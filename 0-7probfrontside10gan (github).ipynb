{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe83af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T17:36:36.717960Z",
     "iopub.status.busy": "2024-04-15T17:36:36.717625Z",
     "iopub.status.idle": "2024-04-15T17:36:51.231827Z",
     "shell.execute_reply": "2024-04-15T17:36:51.230629Z"
    },
    "id": "NRq_t6UZViAQ",
    "outputId": "e6d45250-a588-4e40-83b4-c156611ebe3a",
    "papermill": {
     "duration": 14.527963,
     "end_time": "2024-04-15T17:36:51.234375",
     "exception": false,
     "start_time": "2024-04-15T17:36:36.706412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e30d21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T17:36:51.258005Z",
     "iopub.status.busy": "2024-04-15T17:36:51.257231Z",
     "iopub.status.idle": "2024-04-15T17:37:03.605960Z",
     "shell.execute_reply": "2024-04-15T17:37:03.604549Z"
    },
    "id": "7lIEq0kxIkER",
    "outputId": "a818603d-0207-4d85-cf5f-cde8fe301530",
    "papermill": {
     "duration": 12.363548,
     "end_time": "2024-04-15T17:37:03.608815",
     "exception": false,
     "start_time": "2024-04-15T17:36:51.245267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d882d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T17:37:03.633582Z",
     "iopub.status.busy": "2024-04-15T17:37:03.633142Z",
     "iopub.status.idle": "2024-04-15T17:37:07.581378Z",
     "shell.execute_reply": "2024-04-15T17:37:07.580614Z"
    },
    "id": "kn6J4vqSBB7E",
    "papermill": {
     "duration": 3.96362,
     "end_time": "2024-04-15T17:37:07.583640",
     "exception": false,
     "start_time": "2024-04-15T17:37:03.620020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f\"\"\"\n",
    "Code inspired by:\n",
    "https://github.com/Atze00/MoViNet-pytorch\n",
    "https://pytorch.org/vision/stable/_modules/torchvision/models/mobilenetv2.html\n",
    "https://pytorch.org/vision/stable/_modules/torchvision/models/mobilenetv3.html\n",
    "\"\"\"\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch.nn.modules.utils import _triple, _pair\n",
    "import torch.nn.functional as Ff\n",
    "from typing import Any, Callable, Optional, Tuple, Union\n",
    "from einops import rearrange\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "class Hardsigmoid(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = (0.2 * x + 0.5).clamp(min=0.0, max=1.0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class CausalModule(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.activation = None\n",
    "\n",
    "    def reset_activation(self) -> None:\n",
    "        self.activation = None\n",
    "\n",
    "\n",
    "class TemporalCGAvgPool3D(CausalModule):\n",
    "    def __init__(self,) -> None:\n",
    "        super().__init__()\n",
    "        self.n_cumulated_values = 0\n",
    "        self.register_forward_hook(self._detach_activation)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        input_shape = x.shape\n",
    "        device = x.device\n",
    "        cumulative_sum = torch.cumsum(x, dim=2)\n",
    "        if self.activation is None:\n",
    "            self.activation = cumulative_sum[:, :, -1:].clone()\n",
    "        else:\n",
    "            cumulative_sum += self.activation\n",
    "            self.activation = cumulative_sum[:, :, -1:].clone()\n",
    "        divisor = (torch.arange(1, input_shape[2]+1,\n",
    "                   device=device)[None, None, :, None, None]\n",
    "                   .expand(x.shape))\n",
    "        x = cumulative_sum / (self.n_cumulated_values + divisor)\n",
    "        self.n_cumulated_values += input_shape[2]\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _detach_activation(module: CausalModule,\n",
    "                           input: Tensor,\n",
    "                           output: Tensor) -> None:\n",
    "        module.activation.detach_()\n",
    "\n",
    "    def reset_activation(self) -> None:\n",
    "        super().reset_activation()\n",
    "        self.n_cumulated_values = 0\n",
    "\n",
    "\n",
    "class Conv2dBNActivation(nn.Sequential):\n",
    "    def __init__(\n",
    "                 self,\n",
    "                 in_planes: int,\n",
    "                 out_planes: int,\n",
    "                 *,\n",
    "                 kernel_size: Union[int, Tuple[int, int]],\n",
    "                 padding: Union[int, Tuple[int, int]],\n",
    "                 stride: Union[int, Tuple[int, int]] = 1,\n",
    "                 groups: int = 1,\n",
    "                 norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "                 activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "                 **kwargs: Any,\n",
    "                 ) -> None:\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.Identity\n",
    "        if activation_layer is None:\n",
    "            activation_layer = nn.Identity\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        dict_layers = OrderedDict({\n",
    "                            \"conv2d\": nn.Conv2d(in_planes, out_planes,\n",
    "                                                kernel_size=kernel_size,\n",
    "                                                stride=stride,\n",
    "                                                padding=padding,\n",
    "                                                groups=groups,\n",
    "                                                **kwargs),\n",
    "                            \"norm\": norm_layer(out_planes, eps=0.001),\n",
    "                            \"act\": activation_layer()\n",
    "                            })\n",
    "\n",
    "        self.out_channels = out_planes\n",
    "        super(Conv2dBNActivation, self).__init__(dict_layers)\n",
    "\n",
    "\n",
    "class Conv3DBNActivation(nn.Sequential):\n",
    "    def __init__(\n",
    "                 self,\n",
    "                 in_planes: int,\n",
    "                 out_planes: int,\n",
    "                 *,\n",
    "                 kernel_size: Union[int, Tuple[int, int, int]],\n",
    "                 padding: Union[int, Tuple[int, int, int]],\n",
    "                 stride: Union[int, Tuple[int, int, int]] = 1,\n",
    "                 groups: int = 1,\n",
    "                 norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "                 activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "                 **kwargs: Any,\n",
    "                 ) -> None:\n",
    "        kernel_size = _triple(kernel_size)\n",
    "        stride = _triple(stride)\n",
    "        padding = _triple(padding)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.Identity\n",
    "        if activation_layer is None:\n",
    "            activation_layer = nn.Identity\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        dict_layers = OrderedDict({\n",
    "                                \"conv3d\": nn.Conv3d(in_planes, out_planes,\n",
    "                                                    kernel_size=kernel_size,\n",
    "                                                    stride=stride,\n",
    "                                                    padding=padding,\n",
    "                                                    groups=groups,\n",
    "                                                    **kwargs),\n",
    "                                \"norm\": norm_layer(out_planes, eps=0.001),\n",
    "                                \"act\": activation_layer()\n",
    "                                })\n",
    "\n",
    "        self.out_channels = out_planes\n",
    "        super(Conv3DBNActivation, self).__init__(dict_layers)\n",
    "\n",
    "\n",
    "class ConvBlock3D(CausalModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_planes: int,\n",
    "            out_planes: int,\n",
    "            *,\n",
    "            kernel_size: Union[int, Tuple[int, int, int]],\n",
    "            tf_like: bool,\n",
    "            causal: bool,\n",
    "            conv_type: str,\n",
    "            padding: Union[int, Tuple[int, int, int]] = 0,\n",
    "            stride: Union[int, Tuple[int, int, int]] = 1,\n",
    "            norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "            activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "            bias: bool = False,\n",
    "            **kwargs: Any,\n",
    "            ) -> None:\n",
    "        super().__init__()\n",
    "        kernel_size = _triple(kernel_size)\n",
    "        stride = _triple(stride)\n",
    "        padding = _triple(padding)\n",
    "        self.conv_2 = None\n",
    "        if tf_like:\n",
    "            if kernel_size[0] % 2 == 0:\n",
    "                raise ValueError('tf_like supports only odd'\n",
    "                                 + ' kernels for temporal dimension')\n",
    "            padding = ((kernel_size[0]-1)//2, 0, 0)\n",
    "            if stride[0] != 1:\n",
    "                raise ValueError('illegal stride value, tf like supports'\n",
    "                                 + ' only stride == 1 for temporal dimension')\n",
    "            if stride[1] > kernel_size[1] or stride[2] > kernel_size[2]:\n",
    "                raise ValueError('tf_like supports only'\n",
    "                                 + '  stride <= of the kernel size')\n",
    "\n",
    "        if causal is True:\n",
    "            padding = (0, padding[1], padding[2])\n",
    "        if conv_type != \"2plus1d\" and conv_type != \"3d\":\n",
    "            raise ValueError(\"only 2plus2d or 3d are \"\n",
    "                             + \"allowed as 3d convolutions\")\n",
    "\n",
    "        if conv_type == \"2plus1d\":\n",
    "            self.conv_1 = Conv2dBNActivation(in_planes,\n",
    "                                             out_planes,\n",
    "                                             kernel_size=(kernel_size[1],\n",
    "                                                          kernel_size[2]),\n",
    "                                             padding=(padding[1],\n",
    "                                                      padding[2]),\n",
    "                                             stride=(stride[1], stride[2]),\n",
    "                                             activation_layer=activation_layer,\n",
    "                                             norm_layer=norm_layer,\n",
    "                                             bias=bias,\n",
    "                                             **kwargs)\n",
    "            if kernel_size[0] > 1:\n",
    "                self.conv_2 = Conv2dBNActivation(in_planes,\n",
    "                                                 out_planes,\n",
    "                                                 kernel_size=(kernel_size[0],\n",
    "                                                              1),\n",
    "                                                 padding=(padding[0], 0),\n",
    "                                                 stride=(stride[0], 1),\n",
    "                                                 activation_layer=activation_layer,\n",
    "                                                 norm_layer=norm_layer,\n",
    "                                                 bias=bias,\n",
    "                                                 **kwargs)\n",
    "        elif conv_type == \"3d\":\n",
    "            self.conv_1 = Conv3DBNActivation(in_planes,\n",
    "                                             out_planes,\n",
    "                                             kernel_size=kernel_size,\n",
    "                                             padding=padding,\n",
    "                                             activation_layer=activation_layer,\n",
    "                                             norm_layer=norm_layer,\n",
    "                                             stride=stride,\n",
    "                                             bias=bias,\n",
    "                                             **kwargs)\n",
    "        self.padding = padding\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dim_pad = self.kernel_size[0]-1\n",
    "        self.stride = stride\n",
    "        self.causal = causal\n",
    "        self.conv_type = conv_type\n",
    "        self.tf_like = tf_like\n",
    "\n",
    "    def _forward(self, x: Tensor) -> Tensor:\n",
    "        device = x.device\n",
    "        if self.dim_pad > 0 and self.conv_2 is None and self.causal is True:\n",
    "            x = self._cat_stream_buffer(x, device)\n",
    "        shape_with_buffer = x.shape\n",
    "        if self.conv_type == \"2plus1d\":\n",
    "            x = rearrange(x, \"b c t h w -> (b t) c h w\")\n",
    "        x = self.conv_1(x)\n",
    "        if self.conv_type == \"2plus1d\":\n",
    "            x = rearrange(x,\n",
    "                          \"(b t) c h w -> b c t h w\",\n",
    "                          t=shape_with_buffer[2])\n",
    "\n",
    "            if self.conv_2 is not None:\n",
    "                if self.dim_pad > 0 and self.causal is True:\n",
    "                    x = self._cat_stream_buffer(x, device)\n",
    "                w = x.shape[-1]\n",
    "                x = rearrange(x, \"b c t h w -> b c t (h w)\")\n",
    "                x = self.conv_2(x)\n",
    "                x = rearrange(x, \"b c t (h w) -> b c t h w\", w=w)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.tf_like:\n",
    "            x = same_padding(x, x.shape[-2], x.shape[-1],\n",
    "                             self.stride[-2], self.stride[-1],\n",
    "                             self.kernel_size[-2], self.kernel_size[-1])\n",
    "        x = self._forward(x)\n",
    "        return x\n",
    "\n",
    "    def _cat_stream_buffer(self, x: Tensor, device: torch.device) -> Tensor:\n",
    "        if self.activation is None:\n",
    "            self._setup_activation(x.shape)\n",
    "        x = torch.cat((self.activation.to(device), x), 2)\n",
    "        self._save_in_activation(x)\n",
    "        return x\n",
    "\n",
    "    def _save_in_activation(self, x: Tensor) -> None:\n",
    "        assert self.dim_pad > 0\n",
    "        self.activation = x[:, :, -self.dim_pad:, ...].clone().detach()\n",
    "\n",
    "    def _setup_activation(self, input_shape: Tuple[float, ...]) -> None:\n",
    "        assert self.dim_pad > 0\n",
    "        self.activation = torch.zeros(*input_shape[:2],  \n",
    "                                      self.dim_pad,\n",
    "                                      *input_shape[3:])\n",
    "\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels: int,  \n",
    "                 activation_2: nn.Module,\n",
    "                 activation_1: nn.Module,\n",
    "                 conv_type: str,\n",
    "                 causal: bool,\n",
    "                 squeeze_factor: int = 4,\n",
    "                 bias: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        se_multiplier = 2 if causal else 1\n",
    "        squeeze_channels = _make_divisible(input_channels\n",
    "                                           // squeeze_factor\n",
    "                                           * se_multiplier, 8)\n",
    "        self.temporal_cumualtive_GAvg3D = TemporalCGAvgPool3D()\n",
    "        self.fc1 = ConvBlock3D(input_channels*se_multiplier,\n",
    "                               squeeze_channels,\n",
    "                               kernel_size=(1, 1, 1),\n",
    "                               padding=0,\n",
    "                               tf_like=False,\n",
    "                               causal=causal,\n",
    "                               conv_type=conv_type,\n",
    "                               bias=bias)\n",
    "        self.activation_1 = activation_1()\n",
    "        self.activation_2 = activation_2()\n",
    "        self.fc2 = ConvBlock3D(squeeze_channels,\n",
    "                               input_channels,\n",
    "                               kernel_size=(1, 1, 1),\n",
    "                               padding=0,\n",
    "                               tf_like=False,\n",
    "                               causal=causal,\n",
    "                               conv_type=conv_type,\n",
    "                               bias=bias)\n",
    "\n",
    "    def _scale(self, input: Tensor) -> Tensor:\n",
    "        if self.causal:\n",
    "            x_space = torch.mean(input, dim=[3, 4], keepdim=True)\n",
    "            scale = self.temporal_cumualtive_GAvg3D(x_space)\n",
    "            scale = torch.cat((scale, x_space), dim=1)\n",
    "        else:\n",
    "            scale = F.adaptive_avg_pool3d(input, 1)\n",
    "        scale = self.fc1(scale)\n",
    "        scale = self.activation_1(scale)\n",
    "        scale = self.fc2(scale)\n",
    "        return self.activation_2(scale)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        scale = self._scale(input)\n",
    "        return scale * input\n",
    "\n",
    "\n",
    "def _make_divisible(v: float,\n",
    "                    divisor: int,\n",
    "                    min_value: Optional[int] = None\n",
    "                    ) -> int:\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def same_padding(x: Tensor,\n",
    "                 in_height: int, in_width: int,\n",
    "                 stride_h: int, stride_w: int,\n",
    "                 filter_height: int, filter_width: int) -> Tensor:\n",
    "    if (in_height % stride_h == 0):\n",
    "        pad_along_height = max(filter_height - stride_h, 0)\n",
    "    else:\n",
    "        pad_along_height = max(filter_height - (in_height % stride_h), 0)\n",
    "    if (in_width % stride_w == 0):\n",
    "        pad_along_width = max(filter_width - stride_w, 0)\n",
    "    else:\n",
    "        pad_along_width = max(filter_width - (in_width % stride_w), 0)\n",
    "    pad_top = pad_along_height // 2\n",
    "    pad_bottom = pad_along_height - pad_top\n",
    "    pad_left = pad_along_width // 2\n",
    "    pad_right = pad_along_width - pad_left\n",
    "    padding_pad = (pad_left, pad_right, pad_top, pad_bottom)\n",
    "    return torch.nn.functional.pad(x, padding_pad)\n",
    "\n",
    "\n",
    "class tfAvgPool3D(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.avgf = nn.AvgPool3d((1, 3, 3), stride=(1, 2, 2))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if x.shape[-1] != x.shape[-2]:\n",
    "            raise RuntimeError('only same shape for h and w ' +\n",
    "                               'are supported by avg with tf_like')\n",
    "        if x.shape[-1] != x.shape[-2]:\n",
    "            raise RuntimeError('only same shape for h and w ' +\n",
    "                               'are supported by avg with tf_like')\n",
    "        f1 = x.shape[-1] % 2 != 0\n",
    "        if f1:\n",
    "            padding_pad = (0, 0, 0, 0)\n",
    "        else:\n",
    "            padding_pad = (0, 1, 0, 1)\n",
    "        x = torch.nn.functional.pad(x, padding_pad)\n",
    "        if f1:\n",
    "            x = torch.nn.functional.avg_pool3d(x,\n",
    "                                               (1, 3, 3),\n",
    "                                               stride=(1, 2, 2),\n",
    "                                               count_include_pad=False,\n",
    "                                               padding=(0, 1, 1))\n",
    "        else:\n",
    "            x = self.avgf(x)\n",
    "            x[..., -1] = x[..., -1] * 9/6\n",
    "            x[..., -1, :] = x[..., -1, :] * 9/6\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicBneck(nn.Module):\n",
    "    def __init__(self,\n",
    "                 cfg: \"CfgNode\",\n",
    "                 causal: bool,\n",
    "                 tf_like: bool,\n",
    "                 conv_type: str,\n",
    "                 norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "                 activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        assert type(cfg.stride) is tuple\n",
    "        if (not cfg.stride[0] == 1\n",
    "                or not (1 <= cfg.stride[1] <= 2)\n",
    "                or not (1 <= cfg.stride[2] <= 2)):\n",
    "            raise ValueError('illegal stride value')\n",
    "        self.res = None\n",
    "\n",
    "        layers = []\n",
    "        if cfg.expanded_channels != cfg.out_channels:\n",
    "            # expand\n",
    "            self.expand = ConvBlock3D(\n",
    "                in_planes=cfg.input_channels,\n",
    "                out_planes=cfg.expanded_channels,\n",
    "                kernel_size=(1, 1, 1),\n",
    "                padding=(0, 0, 0),\n",
    "                causal=causal,\n",
    "                conv_type=conv_type,\n",
    "                tf_like=tf_like,\n",
    "                norm_layer=norm_layer,\n",
    "                activation_layer=activation_layer\n",
    "                )\n",
    "\n",
    "        self.deep = ConvBlock3D(\n",
    "            in_planes=cfg.expanded_channels,\n",
    "            out_planes=cfg.expanded_channels,\n",
    "            kernel_size=cfg.kernel_size,\n",
    "            padding=cfg.padding,\n",
    "            stride=cfg.stride,\n",
    "            groups=cfg.expanded_channels,\n",
    "            causal=causal,\n",
    "            conv_type=conv_type,\n",
    "            tf_like=tf_like,\n",
    "            norm_layer=norm_layer,\n",
    "            activation_layer=activation_layer\n",
    "            )\n",
    "\n",
    "        self.se = SqueezeExcitation(cfg.expanded_channels,\n",
    "                                    causal=causal,\n",
    "                                    activation_1=activation_layer,\n",
    "                                    activation_2=(nn.Sigmoid\n",
    "                                                  if conv_type == \"3d\"\n",
    "                                                  else Hardsigmoid),\n",
    "                                    conv_type=conv_type\n",
    "                                    )\n",
    "\n",
    "        self.project = ConvBlock3D(\n",
    "            cfg.expanded_channels,\n",
    "            cfg.out_channels,\n",
    "            kernel_size=(1, 1, 1),\n",
    "            padding=(0, 0, 0),\n",
    "            causal=causal,\n",
    "            conv_type=conv_type,\n",
    "            tf_like=tf_like,\n",
    "            norm_layer=norm_layer,\n",
    "            activation_layer=nn.Identity\n",
    "            )\n",
    "\n",
    "        if not (cfg.stride == (1, 1, 1)\n",
    "                and cfg.input_channels == cfg.out_channels):\n",
    "            if cfg.stride != (1, 1, 1):\n",
    "                if tf_like:\n",
    "                    layers.append(tfAvgPool3D())\n",
    "                else:\n",
    "                    layers.append(nn.AvgPool3d((1, 3, 3),\n",
    "                                  stride=cfg.stride,\n",
    "                                  padding=cfg.padding_avg))\n",
    "            layers.append(ConvBlock3D(\n",
    "                    in_planes=cfg.input_channels,\n",
    "                    out_planes=cfg.out_channels,\n",
    "                    kernel_size=(1, 1, 1),\n",
    "                    padding=(0, 0, 0),\n",
    "                    norm_layer=norm_layer,\n",
    "                    activation_layer=nn.Identity,\n",
    "                    causal=causal,\n",
    "                    conv_type=conv_type,\n",
    "                    tf_like=tf_like\n",
    "                    ))\n",
    "            self.res = nn.Sequential(*layers)\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        if self.res is not None:\n",
    "            residual = self.res(input)\n",
    "        else:\n",
    "            residual = input\n",
    "        if self.expand is not None:\n",
    "            x = self.expand(input)\n",
    "        else:\n",
    "            x = input\n",
    "        x = self.deep(x)\n",
    "        x = self.se(x)\n",
    "        x = self.project(x)\n",
    "        result = residual + self.alpha * x\n",
    "        return result\n",
    "\n",
    "\n",
    "class MoViNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 cfg: \"CfgNode\",\n",
    "                 causal: bool = True,\n",
    "                 pretrained: bool = False,\n",
    "                 num_classes: int = 16,\n",
    "                 conv_type: str = \"3d\",\n",
    "                 tf_like: bool = False\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        causal: causal mode\n",
    "        pretrained: pretrained models\n",
    "        If pretrained is True:\n",
    "            num_classes is set to 600,\n",
    "            conv_type is set to \"3d\" if causal is False,\n",
    "                \"2plus1d\" if causal is True\n",
    "            tf_like is set to True\n",
    "        num_classes: number of classes for classifcation\n",
    "        conv_type: type of convolution either 3d or 2plus1d\n",
    "        tf_like: tf_like behaviour, basically same padding for convolutions\n",
    "        \"\"\"\n",
    "        if pretrained:\n",
    "            tf_like = True\n",
    "            num_classes = 600\n",
    "            conv_type = \"2plus1d\" if causal else \"3d\"\n",
    "        blocks_dic = OrderedDict()\n",
    "\n",
    "        norm_layer = nn.BatchNorm3d if conv_type == \"3d\" else nn.BatchNorm2d\n",
    "        activation_layer = Swish if conv_type == \"3d\" else nn.Hardswish\n",
    "\n",
    "        self.conv1 = ConvBlock3D(\n",
    "            in_planes=cfg.conv1.input_channels,\n",
    "            out_planes=cfg.conv1.out_channels,\n",
    "            kernel_size=cfg.conv1.kernel_size,\n",
    "            stride=cfg.conv1.stride,\n",
    "            padding=cfg.conv1.padding,\n",
    "            causal=causal,\n",
    "            conv_type=conv_type,\n",
    "            tf_like=tf_like,\n",
    "            norm_layer=norm_layer,\n",
    "            activation_layer=activation_layer\n",
    "            )\n",
    "\n",
    "        for i, block in enumerate(cfg.blocks):\n",
    "            for j, basicblock in enumerate(block):\n",
    "                blocks_dic[f\"b{i}_l{j}\"] = BasicBneck(basicblock,\n",
    "                                                      causal=causal,\n",
    "                                                      conv_type=conv_type,\n",
    "                                                      tf_like=tf_like,\n",
    "                                                      norm_layer=norm_layer,\n",
    "                                                      activation_layer=activation_layer\n",
    "                                                      )\n",
    "        self.blocks = nn.Sequential(blocks_dic)\n",
    "      \n",
    "        self.conv7 = ConvBlock3D(\n",
    "            in_planes=cfg.conv7.input_channels,\n",
    "            out_planes=cfg.conv7.out_channels,\n",
    "            kernel_size=cfg.conv7.kernel_size,\n",
    "            stride=cfg.conv7.stride,\n",
    "            padding=cfg.conv7.padding,\n",
    "            causal=causal,\n",
    "            conv_type=conv_type,\n",
    "            tf_like=tf_like,\n",
    "            norm_layer=norm_layer,\n",
    "            activation_layer=activation_layer\n",
    "            )\n",
    "       \n",
    "        self.classifier = nn.Sequential(\n",
    "        \n",
    "            ConvBlock3D(cfg.conv7.out_channels,\n",
    "                        cfg.dense9.hidden_dim,\n",
    "                        kernel_size=(1, 1, 1),\n",
    "                        tf_like=tf_like,\n",
    "                        causal=causal,\n",
    "                        conv_type=conv_type,\n",
    "                        bias=True),\n",
    "            Swish(),\n",
    "            nn.Dropout(p=0.5, inplace=True),\n",
    "      \n",
    "            ConvBlock3D(cfg.dense9.hidden_dim,\n",
    "                        num_classes,\n",
    "                        kernel_size=(1, 1, 1),\n",
    "                        tf_like=tf_like,\n",
    "                        causal=causal,\n",
    "                        conv_type=conv_type,\n",
    "                        bias=True),\n",
    "        )\n",
    "        if causal:\n",
    "            self.cgap = TemporalCGAvgPool3D()\n",
    "        if pretrained:\n",
    "            if causal:\n",
    "                if cfg.name not in [\"A0\", \"A1\", \"A2\"]:\n",
    "                    raise ValueError(\"Only A0,A1,A2 streaming\" +\n",
    "                                     \"networks are available pretrained\")\n",
    "                state_dict = (torch.hub\n",
    "                              .load_state_dict_from_url(cfg.stream_weights))\n",
    "            else:\n",
    "                state_dict = torch.hub.load_state_dict_from_url(cfg.weights)\n",
    "            self.load_state_dict(state_dict)\n",
    "        else:\n",
    "            self.apply(self._weight_init)\n",
    "        self.causal = causal\n",
    "\n",
    "    def avg(self, x: Tensor) -> Tensor:\n",
    "        if self.causal:\n",
    "            avg = F.adaptive_avg_pool3d(x, (x.shape[2], 1, 1))\n",
    "            avg = self.cgap(avg)[:, :, -1:]\n",
    "        else:\n",
    "            avg = F.adaptive_avg_pool3d(x, 1)\n",
    "        return avg\n",
    "\n",
    "    @staticmethod\n",
    "    def _weight_init(m): \n",
    "        if isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, (nn.BatchNorm3d, nn.BatchNorm2d, nn.GroupNorm)):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.avg(x)\n",
    "        x = self.classifier(x)\n",
    "        x = x.flatten(1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean_activation_buffers(m):\n",
    "        if issubclass(type(m), CausalModule):\n",
    "            m.reset_activation()\n",
    "\n",
    "    def clean_activation_buffers(self) -> None:\n",
    "        self.apply(self._clean_activation_buffers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d186ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T17:37:07.606946Z",
     "iopub.status.busy": "2024-04-15T17:37:07.606294Z",
     "iopub.status.idle": "2024-04-15T17:37:25.075815Z",
     "shell.execute_reply": "2024-04-15T17:37:25.074639Z"
    },
    "id": "AHQqBwMTJtS7",
    "outputId": "3f0e3a33-dc01-4ff1-946f-31096a15cc83",
    "papermill": {
     "duration": 17.483371,
     "end_time": "2024-04-15T17:37:25.078249",
     "exception": false,
     "start_time": "2024-04-15T17:37:07.594878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install fvcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e22c2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T17:37:25.105291Z",
     "iopub.status.busy": "2024-04-15T17:37:25.104963Z",
     "iopub.status.idle": "2024-04-15T17:37:25.180504Z",
     "shell.execute_reply": "2024-04-15T17:37:25.179514Z"
    },
    "id": "wW63ZyrSBHLQ",
    "papermill": {
     "duration": 0.092448,
     "end_time": "2024-04-15T17:37:25.183053",
     "exception": false,
     "start_time": "2024-04-15T17:37:25.090605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inspired by\n",
    "https://github.com/PeizeSun/SparseR-CNN/blob/dff4c43a9526a6d0d2480abc833e78a7c29ddb1a/detectron2/config/defaults.py\n",
    "\"\"\"\n",
    "from fvcore.common.config import CfgNode as CN\n",
    "\n",
    "def fill_SE_config(conf, input_channels,\n",
    "                    out_channels,\n",
    "                    expanded_channels,\n",
    "                    kernel_size,\n",
    "                    stride,\n",
    "                    padding,\n",
    "                    padding_avg,\n",
    "):\n",
    "    conf.expanded_channels =expanded_channels\n",
    "    conf.padding_avg= padding_avg\n",
    "    fill_conv(conf,input_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    ")\n",
    "\n",
    "def fill_conv(conf, input_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,):\n",
    "    conf.input_channels = input_channels\n",
    "    conf.out_channels = out_channels\n",
    "    conf.kernel_size = kernel_size\n",
    "    conf.stride = stride\n",
    "    conf.padding = padding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_C = CN()\n",
    "\n",
    "_C.MODEL = CN()\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "#### MoViNetA2 ####\n",
    "###################\n",
    "\n",
    "_C.MODEL.MoViNetA2 = CN()\n",
    "_C.MODEL.MoViNetA2.name = \"A2\"\n",
    "_C.MODEL.MoViNetA2.weights = \"https://github.com/Atze00/MoViNet-pytorch/blob/main/weights/modelA2_statedict_v3?raw=true\"\n",
    "_C.MODEL.MoViNetA2.stream_weights = \"https://github.com/Atze00/MoViNet-pytorch/blob/main/weights/modelA2_stream_statedict_v3?raw=true\"\n",
    "\n",
    "_C.MODEL.MoViNetA2.conv1 = CN()\n",
    "fill_conv(_C.MODEL.MoViNetA2.conv1, 3,16,(1,3,3),(1,2,2),(0,1,1))\n",
    "\n",
    "\n",
    "_C.MODEL.MoViNetA2.blocks = [ [CN() for _ in range(3)],\n",
    "                              [CN() for _ in range(5)],\n",
    "                              [CN() for _ in range(5)],\n",
    "                              [CN() for _ in range(6)],\n",
    "                              [CN() for _ in range(7)]]\n",
    "\n",
    "\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[0][0], 16, 16, 40, (1,5,5), (1,2,2), (0,2,2), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[0][1], 16, 16, 40, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[0][2], 16, 16, 64, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "\n",
    "\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[1][0], 16, 40, 96, (3,3,3), (1,2,2), (1,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[1][1], 40, 40, 120, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[1][2], 40, 40, 96, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[1][3], 40, 40, 96, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[1][4], 40, 40, 120, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "\n",
    "\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[2][0], 40, 72, 240, (5,3,3), (1,2,2), (2,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[2][1], 72, 72, 160, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[2][2], 72, 72, 240, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[2][3], 72, 72, 192, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[2][4], 72, 72, 240, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "\n",
    "\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[3][0], 72, 72, 240, (5,3,3), (1,1,1), (2,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[3][1], 72, 72, 240, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[3][2], 72, 72, 240, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[3][3], 72, 72, 240, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[3][4], 72, 72, 144, (1,5,5), (1,1,1), (0,2,2), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[3][5], 72, 72, 240, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "\n",
    "\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[4][0], 72 , 144, 480, (5,3,3), (1,2,2), (2,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[4][1], 144, 144, 384, (1,5,5), (1,1,1), (0,2,2), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[4][2], 144, 144, 384, (1,5,5), (1,1,1), (0,2,2), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[4][3], 144, 144, 480, (1,5,5), (1,1,1), (0,2,2), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[4][4], 144, 144, 480, (1,5,5), (1,1,1), (0,2,2), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[4][5], 144, 144, 480, (3,3,3), (1,1,1), (1,1,1), (0,1,1))\n",
    "fill_SE_config(_C.MODEL.MoViNetA2.blocks[4][6], 144, 144, 576, (1,3,3), (1,1,1), (0,1,1), (0,1,1))\n",
    "\n",
    "_C.MODEL.MoViNetA2.conv7= CN()\n",
    "fill_conv(_C.MODEL.MoViNetA2.conv7, 144,640,(1,1,1),(1,1,1),(0,0,0))\n",
    "\n",
    "_C.MODEL.MoViNetA2.dense9= CN()\n",
    "_C.MODEL.MoViNetA2.dense9.hidden_dim = 2048\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d51ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T17:37:25.209414Z",
     "iopub.status.busy": "2024-04-15T17:37:25.209105Z",
     "iopub.status.idle": "2024-04-15T17:37:43.445387Z",
     "shell.execute_reply": "2024-04-15T17:37:43.444626Z"
    },
    "papermill": {
     "duration": 18.252086,
     "end_time": "2024-04-15T17:37:43.447734",
     "exception": false,
     "start_time": "2024-04-15T17:37:25.195648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "tensorflow_device = \"/device:GPU:0\"\n",
    "def get_file_paths(directory):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "    return file_paths\n",
    "with tf.device(tensorflow_device):\n",
    "    #path to style images dataset\n",
    "    dataset_directory = '/kaggle/input/gan10-gan10/gan_10'\n",
    "    file_paths = get_file_paths(dataset_directory)\n",
    "    #NST model used for style transfer\n",
    "    hub_module = hub.load('https://kaggle.com/models/google/arbitrary-image-stylization-v1/frameworks/TensorFlow1/variations/256/versions/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a746130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T17:37:43.473820Z",
     "iopub.status.busy": "2024-04-15T17:37:43.473524Z",
     "iopub.status.idle": "2024-04-15T17:37:43.673999Z",
     "shell.execute_reply": "2024-04-15T17:37:43.673260Z"
    },
    "papermill": {
     "duration": 0.215975,
     "end_time": "2024-04-15T17:37:43.676268",
     "exception": false,
     "start_time": "2024-04-15T17:37:43.460293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "def nstAugment(nst_model, style_list, prob, gan_dim, output_dim, frames_front, frames_side, seed):\n",
    "    \"\"\"NST based augmentation.\n",
    "    \n",
    "    Args:\n",
    "    nst_model: NST model to be used for augmentation.\n",
    "    style_list: A list containing the path of all the style images.\n",
    "    prob: Probability of augmentation.\n",
    "    gan_dim: Input dimension of NST model.\n",
    "    output_dim: Required dimension for the frame.\n",
    "    frames_front: List of frames to be augmented (Front View).\n",
    "    frames_side: List of frames to be augmented (Side View).\n",
    "    seed: Random seed to ensure uniformity of augmentations\n",
    "    \n",
    "    Returns:\n",
    "    Augmented clips.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    rand = np.random.random(1)\n",
    "    if rand < prob:\n",
    "        # Select a random style image\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        rand_index = np.random.randint(0, len(style_list))\n",
    "        style_image = plt.imread(style_list[rand_index])\n",
    "        \n",
    "        # Preprocess the style image\n",
    "        style_image = style_image.astype(np.float32)[np.newaxis, ...] / 255.\n",
    "        style_image = tf.image.resize(style_image, (gan_dim, gan_dim))\n",
    "        \n",
    "        # Preprocess the frame and apply style transfer\n",
    "        frames_front = [\n",
    "            cv2.resize(\n",
    "                np.squeeze(\n",
    "                    hub_module(\n",
    "                        tf.constant(\n",
    "                            tf.image.resize(\n",
    "                                tf.cast(frame, tf.float32)[tf.newaxis, ...] / 255,\n",
    "                                (gan_dim, gan_dim)\n",
    "                            )\n",
    "                        ),\n",
    "                        tf.constant(style_image)\n",
    "                    )\n",
    "                ),\n",
    "                (output_dim, output_dim)\n",
    "            ) for frame in frames_front\n",
    "        ]\n",
    "        frames_side = [\n",
    "            cv2.resize(\n",
    "                np.squeeze(\n",
    "                    hub_module(\n",
    "                        tf.constant(\n",
    "                            tf.image.resize(\n",
    "                                tf.cast(frame, tf.float32)[tf.newaxis, ...] / 255,\n",
    "                                (gan_dim, gan_dim)\n",
    "                            )\n",
    "                        ),\n",
    "                        tf.constant(style_image)\n",
    "                    )\n",
    "                ),\n",
    "                (output_dim, output_dim)\n",
    "            )\n",
    "            for frame in frames_side\n",
    "        ]\n",
    "        # Convert frame to numpy array\n",
    "        frames_front = np.array(frames_front)\n",
    "        frames_side = np.array(frames_side)\n",
    "        \n",
    "        frames_front = np.squeeze(frames_front)\n",
    "        frames_side = np.squeeze(frames_side)\n",
    "        return frames_front, frames_side\n",
    "    else:\n",
    "        # If augmentation is not applied, return the original frame\n",
    "        frames_front  = np.array(frames_front)\n",
    "        frames_side = np.array(frames_side)\n",
    "        return frames_front, frames_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51960a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T17:37:43.702321Z",
     "iopub.status.busy": "2024-04-15T17:37:43.702053Z",
     "iopub.status.idle": "2024-04-15T17:37:45.280782Z",
     "shell.execute_reply": "2024-04-15T17:37:45.279983Z"
    },
    "papermill": {
     "duration": 1.594412,
     "end_time": "2024-04-15T17:37:45.283070",
     "exception": false,
     "start_time": "2024-04-15T17:37:43.688658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "class FrontSideVideoDataset(Dataset):\n",
    "    def __init__(self, side_data_dir, front_data_dir, num_frames, frame_rate=1, step_between_clips=0, fold=1, train=True, transform=None,nst = None):\n",
    "        self.front_data_dir = front_data_dir\n",
    "        self.side_data_dir = side_data_dir\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_rate = frame_rate\n",
    "        self.step_between_clips = step_between_clips\n",
    "        self.fold = fold\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.nst = nst\n",
    "\n",
    "        # Get a list of video file paths in both front and side directories\n",
    "        self.front_video_paths = self._get_video_paths(self.front_data_dir)\n",
    "        self.side_video_paths = self._get_video_paths(self.side_data_dir)\n",
    "\n",
    "        # Determine the total number of video clips\n",
    "        self.total_clips = min(len(self.front_video_paths), len(self.side_video_paths))\n",
    "\n",
    "    def _get_video_paths(self, data_dir):\n",
    "        video_paths = []\n",
    "        for class_name in os.listdir(data_dir):\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            for filename in os.listdir(class_dir):\n",
    "\n",
    "                if filename.endswith(\".avi\"):  \n",
    "                    video_paths.append((os.path.join(class_dir, filename),int(class_name)))\n",
    "        video_paths.sort()\n",
    "        \n",
    "        return video_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_clips\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            # Get video paths for the front and side views\n",
    "            front_video_path , label1 = self.front_video_paths[idx]\n",
    "            side_video_path , label2 = self.side_video_paths[idx]\n",
    "           \n",
    "            if label1!=label2:\n",
    "                print(\"label mismatch\")\n",
    "                print(label1,label2)\n",
    "            # Read video frames for front view\n",
    "            front_frames = self._read_video_frames(front_video_path)\n",
    "\n",
    "            # Read video frames for side view\n",
    "            side_frames = self._read_video_frames(side_video_path)\n",
    "\n",
    "            # Apply transformations to each frame (if specified) \n",
    "            seed = np.random.randint(2147483647) \n",
    "\n",
    "            #Apply nst\n",
    "            if self.nst:\n",
    "                \n",
    "                front_frames, side_frames = self.nst(frames_front=front_frames, frames_side=side_frames, seed=seed)\n",
    "\n",
    "            #Apply other basic image augmentation methods\n",
    "            if self.transform:\n",
    "                torch.manual_seed(seed)\n",
    "                random.seed(seed)\n",
    "                front_frames = [self.transform(frame) for frame in front_frames]\n",
    "                torch.manual_seed(seed)\n",
    "                random.seed(seed)\n",
    "                side_frames = [self.transform(frame) for frame in side_frames]\n",
    "                    \n",
    "                    \n",
    "\n",
    "            # Stack frames into tensors\n",
    "            front_video_tensor = torch.stack(front_frames)\n",
    "            side_video_tensor = torch.stack(side_frames)\n",
    "\n",
    "            front_video_tensor = front_video_tensor.permute(1,0,2,3)\n",
    "            side_video_tensor = side_video_tensor.permute(1,0,2,3)\n",
    "\n",
    "            # Assuming you want to return both front and side views\n",
    "            return side_video_tensor, front_video_tensor , label1\n",
    "\n",
    "\n",
    "    def _read_video_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        start_frame_idx = 0\n",
    "\n",
    "        for frame_idx in range(start_frame_idx, total_frames):\n",
    "            if frame_idx >= total_frames:\n",
    "                break\n",
    "\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) >= self.num_frames:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e03eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T17:37:45.309722Z",
     "iopub.status.busy": "2024-04-15T17:37:45.308973Z",
     "iopub.status.idle": "2024-04-15T17:37:48.991284Z",
     "shell.execute_reply": "2024-04-15T17:37:48.990494Z"
    },
    "papermill": {
     "duration": 3.697866,
     "end_time": "2024-04-15T17:37:48.993577",
     "exception": false,
     "start_time": "2024-04-15T17:37:45.295711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(97)\n",
    "num_frames = 30 \n",
    "clip_steps = 0\n",
    "Bs_Train = 2\n",
    "Bs_Test = 2\n",
    "\n",
    "gan_model = hub_module\n",
    "style_list = file_paths\n",
    "prob = 0.7\n",
    "input_gan = 256\n",
    "frame_dim = 224\n",
    "\n",
    "\n",
    "transform = v2.Compose([\n",
    "\n",
    "                                 v2.ToTensor(),\n",
    "                                 v2.RandomErasing(p=0.5,scale=(0.01,0.08),ratio=(0.3,3,3),value=0,inplace=False),\n",
    "                                 v2.Normalize(mean=[0, 0, 0], std=[255, 255, 255])\n",
    "                      ])\n",
    "transform_test = v2.Compose([\n",
    "                                 v2.ToTensor(),\n",
    "                                 v2.Normalize(mean=[0, 0, 0], std=[255, 255, 255])\n",
    "                          ])\n",
    "\n",
    "#Creating the nst instance\n",
    "nst_instance = partial(nstAugment, gan_model, style_list, prob, input_gan, frame_dim) \n",
    "train_data_dir_side = '/kaggle/input/3mdad-day-new/3mdad side 30/3mdad side 30/Train 30'\n",
    "train_data_dir_front = '/kaggle/input/3mdad-day-new/3mdad front 30/3mdad front 30/Train 30'\n",
    "train_dataset = FrontSideVideoDataset(train_data_dir_side,train_data_dir_front,\n",
    "                                   num_frames,\n",
    "                                   frame_rate=30,\n",
    "                                   step_between_clips = clip_steps,\n",
    "                                   fold=1,\n",
    "                                   train=True,\n",
    "                                   transform=transform,\n",
    "                                   nst = nst_instance\n",
    "                                   )\n",
    "val_data_dir_side = '/kaggle/input/3mdad-day-new/3mdad side 30/3mdad side 30/Validation 30'\n",
    "val_data_dir_front = '/kaggle/input/3mdad-day-new/3mdad front 30/3mdad front 30/Validation 30'\n",
    "val_dataset = FrontSideVideoDataset(val_data_dir_side,val_data_dir_front,\n",
    "                                   num_frames,\n",
    "                                   frame_rate=30,\n",
    "                                   step_between_clips = clip_steps,\n",
    "                                   fold=1,\n",
    "                                   train=False,\n",
    "                                   transform=transform_test,\n",
    "                                   nst = None\n",
    "                                   )\n",
    "train_loader = DataLoader(train_dataset, batch_size=Bs_Train, shuffle=True, num_workers=0)\n",
    "val_loader  = DataLoader(val_dataset, batch_size=Bs_Test, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edbb075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T17:37:49.020308Z",
     "iopub.status.busy": "2024-04-15T17:37:49.019619Z",
     "iopub.status.idle": "2024-04-15T17:38:01.281574Z",
     "shell.execute_reply": "2024-04-15T17:38:01.280449Z"
    },
    "papermill": {
     "duration": 12.277779,
     "end_time": "2024-04-15T17:38:01.283971",
     "exception": false,
     "start_time": "2024-04-15T17:37:49.006192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install adabelief-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a1263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T17:38:01.311426Z",
     "iopub.status.busy": "2024-04-15T17:38:01.311098Z",
     "iopub.status.idle": "2024-04-15T17:38:01.333587Z",
     "shell.execute_reply": "2024-04-15T17:38:01.332589Z"
    },
    "papermill": {
     "duration": 0.038516,
     "end_time": "2024-04-15T17:38:01.335550",
     "exception": false,
     "start_time": "2024-04-15T17:38:01.297034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculating classwise weights\n",
    "path='/kaggle/input/3mdad-day-new/3mdad front 30/3mdad front 30/Train 30'\n",
    "clas=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in os.listdir(path):\n",
    "    for j in os.listdir(os.path.join(path,i)):\n",
    "        clas[int(i)]+=1\n",
    "sum_=0\n",
    "for i in clas:\n",
    "    sum_+=i\n",
    "print(sum_)\n",
    "\n",
    "class_wghts=[]\n",
    "for i in range(16):\n",
    "    class_wghts.append(sum_/(clas[i]*16))\n",
    "    \n",
    "class_wghts = torch.tensor(class_wghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4551a133",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T17:38:01.362349Z",
     "iopub.status.busy": "2024-04-15T17:38:01.362006Z",
     "iopub.status.idle": "2024-04-15T17:38:01.371988Z",
     "shell.execute_reply": "2024-04-15T17:38:01.371119Z"
    },
    "papermill": {
     "duration": 0.025791,
     "end_time": "2024-04-15T17:38:01.373960",
     "exception": false,
     "start_time": "2024-04-15T17:38:01.348169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MetaLearner(nn.Module):\n",
    "    pytorch_device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    def __init__(self, expert_side, expert_front):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.expert_side = expert_side\n",
    "        self.expert_front = expert_front\n",
    "        self.fc1 = nn.Linear(1200, 64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "\n",
    "    def forward(self, side, front):\n",
    "        pytorch_device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.expert_side.clean_activation_buffers()\n",
    "        self.expert_front.clean_activation_buffers()\n",
    "        \n",
    "        predictions_model1 = self.expert_side(side.to(pytorch_device))\n",
    "        predictions_model2 = self.expert_front(front.to(pytorch_device))\n",
    "        \n",
    "        predictions_from_pretrained_models = torch.cat((predictions_model1, predictions_model2), dim=1)\n",
    "        \n",
    "        hidden = F.relu(self.fc1(predictions_from_pretrained_models))\n",
    "        if self.training:\n",
    "            hidden = self.dropout(hidden)\n",
    "        ensemble_preds = self.fc2(hidden)\n",
    "        out = F.log_softmax(ensemble_preds, dim=1)\n",
    "        \n",
    "        self.expert_side.clean_activation_buffers()\n",
    "        self.expert_front.clean_activation_buffers()\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b9e4c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T17:38:01.400885Z",
     "iopub.status.busy": "2024-04-15T17:38:01.400601Z",
     "iopub.status.idle": "2024-04-15T17:38:01.417050Z",
     "shell.execute_reply": "2024-04-15T17:38:01.416374Z"
    },
    "papermill": {
     "duration": 0.032113,
     "end_time": "2024-04-15T17:38:01.418842",
     "exception": false,
     "start_time": "2024-04-15T17:38:01.386729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SymmetricCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.1, beta=1.0,class_weights=None):\n",
    "        super(SymmetricCrossEntropyLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.class_weights = class_weights \n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, weight=self.class_weights)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.beta * ce_loss\n",
    "        return focal_loss\n",
    "\n",
    "def train_iter(model, data_load, optimizer, loss_val, class_weights):\n",
    "    pytorch_device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    samples = len(data_load.dataset)\n",
    "    model.train()\n",
    "    model.to(pytorch_device)\n",
    "    optimizer.zero_grad()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (side, front, target) in enumerate(data_load):\n",
    "        # Forward pass\n",
    "        out = model(side.to(pytorch_device), front.to(pytorch_device))\n",
    "        # Calculate loss (using symmetric cross entropy)\n",
    "        loss = SymmetricCrossEntropyLoss(class_weights=class_weights)(out, target.to(pytorch_device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target.to(pytorch_device)).sum().item()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('[' + '{:5}'.format(i * len(side)) + '/' + '{:5}'.format(samples) +\n",
    "                  ' (' + '{:3.0f}'.format(100 * i / len(data_load)) + '%)]  Loss: ' +\n",
    "                  '{:6.4f}'.format(loss.item()))\n",
    "            loss_val.append(loss.item())\n",
    "\n",
    "    # Calculate and print training accuracy within the loop\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Training Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "def evaluate(model, data_load, loss_val, class_weights):\n",
    "    model.eval()\n",
    "    pytorch_device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    samples = len(data_load.dataset)\n",
    "    csamp = 0\n",
    "    tloss = 0\n",
    "    with torch.no_grad():\n",
    "        for side, front, target in data_load:\n",
    "            out = model(side.to(pytorch_device), front.to(pytorch_device))\n",
    "            # Calculate loss (using symmetric cross entropy)\n",
    "            loss = SymmetricCrossEntropyLoss(class_weights=class_weights)(out, target.to(pytorch_device))\n",
    "            _, pred = torch.max(out, dim=1)\n",
    "            tloss += loss.item()\n",
    "            csamp += pred.eq(target.to(pytorch_device)).sum()\n",
    "    aloss = tloss / samples\n",
    "    loss_val.append(aloss)\n",
    "    print('\\nAverage test loss: ' + '{:.4f}'.format(aloss) +\n",
    "          '  Accuracy:' + '{:5}'.format(csamp) + '/' +\n",
    "          '{:5}'.format(samples) + '(' +\n",
    "          '{:4.2f}'.format(100.0 * csamp / samples) + '%)\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98866396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-15T17:38:01.446518Z",
     "iopub.status.busy": "2024-04-15T17:38:01.446135Z",
     "iopub.status.idle": "2024-04-16T04:26:12.608142Z",
     "shell.execute_reply": "2024-04-16T04:26:12.607119Z"
    },
    "papermill": {
     "duration": 38891.206358,
     "end_time": "2024-04-16T04:26:12.638206",
     "exception": false,
     "start_time": "2024-04-15T17:38:01.431848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from adabelief_pytorch import AdaBelief\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "expert_side = MoViNet(_C.MODEL.MoViNetA2, causal=False, pretrained=True)  # Instantiate and load pre-trained model 1\n",
    "\n",
    "expert_front = MoViNet(_C.MODEL.MoViNetA2, causal=False, pretrained=True)  # Instantiate and load pre-trained model 2\n",
    "\n",
    "pytorch_device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "with torch.cuda.device(pytorch_device):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = \"/kaggle/working/best_model.pth\"\n",
    "\n",
    "    num_epochs = 14\n",
    "    traccuracy_val = []\n",
    "\n",
    "    # Initialize lists to track training and val losses\n",
    "    trloss, tvloss = [], [] \n",
    "\n",
    "    meta_learner = MetaLearner(expert_side, expert_front)\n",
    "    # Move the meta-learner to the GPU \n",
    "    meta_learner.to(pytorch_device)  # Initialize the meta-learner\n",
    "    meta_learner.train()  # Set the meta-learner to training mode\n",
    "    meta_learner.load_state_dict(torch.load(f='/kaggle/input/0-7probfrontside10ganrun1/best_model (8).pth'))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Use AdaBelief optimizer\n",
    "    optimizer = AdaBelief(meta_learner.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-2, weight_decouple=True, rectify=True)\n",
    "    scheduler=ReduceLROnPlateau(optimizer,mode='min',factor=0.5, patience=3, verbose=True)\n",
    "    patience = 5  # Number of epochs with no improvement to wait before stopping\n",
    "    early_stopping_counter = 0\n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print('Epoch:', epoch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_iter(meta_learner, train_loader, optimizer, trloss,class_wghts.to(pytorch_device))\n",
    "        evaluate(meta_learner, val_loader, tvloss,class_wghts.to(pytorch_device))\n",
    "        current_val_loss = tvloss[-1]\n",
    "\n",
    "        scheduler.step(current_val_loss)\n",
    "\n",
    "        if current_val_loss < best_val_loss:\n",
    "            best_val_loss = current_val_loss\n",
    "            # Save the model with the best validation loss\n",
    "            torch.save(meta_learner.state_dict(), best_model_path)\n",
    "\n",
    "            # Reset the early stopping counter\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f'Early stopping triggered after {patience} epochs without improvement.')\n",
    "            break\n",
    "\n",
    "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6be592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-13T19:13:14.495009Z",
     "iopub.status.busy": "2024-04-13T19:13:14.494370Z",
     "iopub.status.idle": "2024-04-13T19:13:14.565136Z",
     "shell.execute_reply": "2024-04-13T19:13:14.564108Z",
     "shell.execute_reply.started": "2024-04-13T19:13:14.494982Z"
    },
    "papermill": {
     "duration": 0.028629,
     "end_time": "2024-04-16T04:26:12.694962",
     "exception": false,
     "start_time": "2024-04-16T04:26:12.666333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a479fbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:26:12.752647Z",
     "iopub.status.busy": "2024-04-16T04:26:12.752322Z",
     "iopub.status.idle": "2024-04-16T04:26:12.757665Z",
     "shell.execute_reply": "2024-04-16T04:26:12.756750Z"
    },
    "papermill": {
     "duration": 0.036543,
     "end_time": "2024-04-16T04:26:12.759603",
     "exception": false,
     "start_time": "2024-04-16T04:26:12.723060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb2042",
   "metadata": {
    "papermill": {
     "duration": 0.028307,
     "end_time": "2024-04-16T04:26:12.816926",
     "exception": false,
     "start_time": "2024-04-16T04:26:12.788619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c2cc64",
   "metadata": {
    "papermill": {
     "duration": 0.028699,
     "end_time": "2024-04-16T04:26:12.873994",
     "exception": false,
     "start_time": "2024-04-16T04:26:12.845295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3881c2",
   "metadata": {
    "papermill": {
     "duration": 0.02834,
     "end_time": "2024-04-16T04:26:12.931310",
     "exception": false,
     "start_time": "2024-04-16T04:26:12.902970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29aa96e",
   "metadata": {
    "papermill": {
     "duration": 0.028168,
     "end_time": "2024-04-16T04:26:12.987982",
     "exception": false,
     "start_time": "2024-04-16T04:26:12.959814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce0feb",
   "metadata": {
    "papermill": {
     "duration": 0.029234,
     "end_time": "2024-04-16T04:26:13.046935",
     "exception": false,
     "start_time": "2024-04-16T04:26:13.017701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1c108",
   "metadata": {
    "papermill": {
     "duration": 0.028254,
     "end_time": "2024-04-16T04:26:13.103873",
     "exception": false,
     "start_time": "2024-04-16T04:26:13.075619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef84ce",
   "metadata": {
    "papermill": {
     "duration": 0.02836,
     "end_time": "2024-04-16T04:26:13.161231",
     "exception": false,
     "start_time": "2024-04-16T04:26:13.132871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9637a1dd",
   "metadata": {
    "papermill": {
     "duration": 0.028625,
     "end_time": "2024-04-16T04:26:13.218689",
     "exception": false,
     "start_time": "2024-04-16T04:26:13.190064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137be275",
   "metadata": {
    "papermill": {
     "duration": 0.028977,
     "end_time": "2024-04-16T04:26:13.276190",
     "exception": false,
     "start_time": "2024-04-16T04:26:13.247213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a9a690",
   "metadata": {
    "papermill": {
     "duration": 0.028725,
     "end_time": "2024-04-16T04:26:13.333675",
     "exception": false,
     "start_time": "2024-04-16T04:26:13.304950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03ab1d",
   "metadata": {
    "papermill": {
     "duration": 0.02845,
     "end_time": "2024-04-16T04:26:13.390644",
     "exception": false,
     "start_time": "2024-04-16T04:26:13.362194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4180468,
     "sourceId": 7222342,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4239862,
     "sourceId": 7307331,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4240481,
     "sourceId": 7308227,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4240891,
     "sourceId": 7308825,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4251476,
     "sourceId": 7325067,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4256475,
     "sourceId": 7332305,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4309247,
     "sourceId": 7409062,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4318769,
     "sourceId": 7422708,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4335995,
     "sourceId": 7449095,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4339187,
     "sourceId": 7454737,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4353526,
     "sourceId": 7479160,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4354107,
     "sourceId": 7479930,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4356221,
     "sourceId": 7483170,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4362225,
     "sourceId": 7492170,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4364573,
     "sourceId": 7495760,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4371968,
     "sourceId": 7507000,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4375000,
     "sourceId": 7511616,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4378820,
     "sourceId": 7517455,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4378956,
     "sourceId": 7517652,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4384932,
     "sourceId": 7528388,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4384955,
     "sourceId": 7528438,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4393052,
     "sourceId": 7543828,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4395844,
     "sourceId": 7547768,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4511249,
     "sourceId": 7722630,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4511283,
     "sourceId": 7722668,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4513892,
     "sourceId": 7726200,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4523172,
     "sourceId": 7738979,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4523392,
     "sourceId": 7739292,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4526400,
     "sourceId": 7743636,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4529859,
     "sourceId": 7748555,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4533146,
     "sourceId": 7752909,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4536041,
     "sourceId": 7757102,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4781251,
     "sourceId": 8097409,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4785284,
     "sourceId": 8102701,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4791224,
     "sourceId": 8110825,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4793312,
     "sourceId": 8113717,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4797753,
     "sourceId": 8119788,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4804164,
     "sourceId": 8128484,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 949,
     "sourceId": 1102,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38983.223639,
   "end_time": "2024-04-16T04:26:17.217696",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-15T17:36:33.994057",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
